{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern GenAI RAG System with Databricks - Production Ready\n",
    "This notebook implements a complete RAG system using Databricks modern stack including Mosaic AI, Vector DB, Agent Framework, and comprehensive monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Parameters\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    # Service Principal Configuration\n",
    "    service_principal_id: str = \"YOUR_SERVICE_PRINCIPAL_ID\"\n",
    "    service_principal_secret: str = \"YOUR_SERVICE_PRINCIPAL_SECRET\"\n",
    "    \n",
    "    # Azure OpenAI Configuration\n",
    "    azure_openai_endpoint: str = \"YOUR_AZURE_OPENAI_ENDPOINT\"\n",
    "    azure_openai_key: str = \"YOUR_AZURE_OPENAI_KEY\"\n",
    "    azure_openai_model: str = \"gpt-4\"\n",
    "    azure_openai_api_version: str = \"2024-02-15-preview\"\n",
    "    \n",
    "    # Databricks Configuration\n",
    "    catalog_name: str = \"rag_catalog\"\n",
    "    schema_name: str = \"rag_schema\"\n",
    "    volume_name: str = \"rag_volume\"\n",
    "    table_name: str = \"diabetes_faq_table\"\n",
    "    vector_index_name: str = \"diabetes_faq_vector_index\"\n",
    "    \n",
    "    # Mosaic AI Configuration\n",
    "    model_serving_endpoint: str = \"rag-model-endpoint\"\n",
    "    vector_search_endpoint: str = \"vector-search-endpoint\"\n",
    "    embedding_model: str = \"databricks-gte-large-en\"\n",
    "    \n",
    "    # Serverless Configuration\n",
    "    serverless_warehouse: str = \"rag-serverless-warehouse\"\n",
    "    serverless_job_name: str = \"rag-etl-pipeline\"\n",
    "    \n",
    "    # Environment Configuration\n",
    "    environment: str = \"dev\"  # dev, qa, prod\n",
    "    provisioned_throughput: Optional[int] = None  # Set for prod\n",
    "    \n",
    "    # Monitoring Configuration\n",
    "    inference_table_name: str = \"model_inference_logs\"\n",
    "    monitoring_dashboard_id: str = \"rag-monitoring-dashboard\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = RAGConfig()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Handling and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import traceback\n",
    "from typing import Callable, Any\n",
    "\n",
    "class RAGException(Exception):\n",
    "    \"\"\"Custom exception for RAG operations\"\"\"\n",
    "    pass\n",
    "\n",
    "def error_handler(func: Callable) -> Callable:\n",
    "    \"\"\"Decorator for error handling\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in {func.__name__}: {str(e)}\")\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            raise RAGException(f\"Failed to execute {func.__name__}: {str(e)}\")\n",
    "    return wrapper\n",
    "\n",
    "def validate_config(config: RAGConfig) -> bool:\n",
    "    \"\"\"Validate configuration parameters\"\"\"\n",
    "    required_fields = [\n",
    "        'service_principal_id', 'azure_openai_endpoint', \n",
    "        'catalog_name', 'schema_name'\n",
    "    ]\n",
    "    \n",
    "    for field in required_fields:\n",
    "        if not getattr(config, field) or getattr(config, field).startswith(\"YOUR_\"):\n",
    "            raise RAGException(f\"Configuration field '{field}' is not properly set\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Validate configuration\n",
    "validate_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Service Principal Authentication Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import *\n",
    "\n",
    "@error_handler\n",
    "def setup_service_principal_auth():\n",
    "    \"\"\"Setup service principal authentication\"\"\"\n",
    "    # Initialize Databricks workspace client with service principal\n",
    "    w = WorkspaceClient(\n",
    "        host=spark.conf.get(\"spark.databricks.workspaceUrl\"),\n",
    "        client_id=config.service_principal_id,\n",
    "        client_secret=config.service_principal_secret\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Service principal authentication configured successfully\")\n",
    "    return w\n",
    "\n",
    "# Initialize workspace client\n",
    "workspace_client = setup_service_principal_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unity Catalog and Volumes Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def setup_unity_catalog():\n",
    "    \"\"\"Setup Unity Catalog components\"\"\"\n",
    "    \n",
    "    # Create catalog if not exists\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {config.catalog_name}\")\n",
    "        logger.info(f\"Catalog {config.catalog_name} created/verified\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Catalog creation warning: {e}\")\n",
    "    \n",
    "    # Create schema if not exists\n",
    "    try:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {config.catalog_name}.{config.schema_name}\")\n",
    "        logger.info(f\"Schema {config.schema_name} created/verified\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Schema creation warning: {e}\")\n",
    "    \n",
    "    # Create volume for unstructured data\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE VOLUME IF NOT EXISTS {config.catalog_name}.{config.schema_name}.{config.volume_name}\n",
    "        \"\"\")\n",
    "        logger.info(f\"Volume {config.volume_name} created/verified\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Volume creation warning: {e}\")\n",
    "\n",
    "# Setup Unity Catalog\n",
    "setup_unity_catalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading and ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import uuid\n",
    "\n",
    "@error_handler\n",
    "def load_and_process_data():\n",
    "    \"\"\"Load and process data with proper error handling\"\"\"\n",
    "    \n",
    "    # Create volume path for data storage\n",
    "    volume_path = f\"/Volumes/{config.catalog_name}/{config.schema_name}/{config.volume_name}\"\n",
    "    \n",
    "    # Download data to volume\n",
    "    dbutils.fs.mkdirs(volume_path)\n",
    "    \n",
    "    # Download CSV file\n",
    "    import urllib.request\n",
    "    csv_url = \"https://raw.githubusercontent.com/kuljotSB/DatabricksUdemyCourse/refs/heads/main/GenAI/diabetes_treatment_faq.csv\"\n",
    "    local_file_path = f\"{volume_path}/diabetes_faq.csv\"\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(csv_url, f\"/dbfs{local_file_path}\")\n",
    "        logger.info(f\"Data downloaded to {local_file_path}\")\n",
    "    except Exception as e:\n",
    "        raise RAGException(f\"Failed to download data: {e}\")\n",
    "    \n",
    "    # Load data with schema validation\n",
    "    df_schema = StructType([\n",
    "        StructField(\"Topic\", StringType(), True),\n",
    "        StructField(\"Description\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.read.schema(df_schema).option(\"header\", \"true\").csv(local_file_path)\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df = df.withColumn(\"record_id\", expr(\"uuid()\")) \\\n",
    "          .withColumn(\"created_at\", current_timestamp()) \\\n",
    "          .withColumn(\"updated_at\", current_timestamp())\n",
    "    \n",
    "    # Data validation\n",
    "    if df.count() == 0:\n",
    "        raise RAGException(\"No data found in the source file\")\n",
    "    \n",
    "    logger.info(f\"Loaded {df.count()} records\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and process data\n",
    "df = load_and_process_data()\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Serverless Warehouse Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def setup_serverless_warehouse():\n",
    "    \"\"\"Setup serverless warehouse for structured data queries\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create serverless warehouse using SQL\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE WAREHOUSE IF NOT EXISTS {config.serverless_warehouse}\n",
    "            WITH (\n",
    "                WAREHOUSE_SIZE = 'MEDIUM',\n",
    "                AUTO_STOP_MINS = 10,\n",
    "                SCALING_POLICY = 'ECONOMY'\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"Serverless warehouse {config.serverless_warehouse} configured\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Warehouse setup warning: {e}\")\n",
    "\n",
    "# Setup serverless warehouse\n",
    "setup_serverless_warehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Delta Table Creation with Change Data Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def create_delta_table(df):\n",
    "    \"\"\"Create Delta table with change data feed enabled\"\"\"\n",
    "    \n",
    "    table_path = f\"{config.catalog_name}.{config.schema_name}.{config.table_name}\"\n",
    "    \n",
    "    # Write DataFrame as Delta table\n",
    "    (df.write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"delta.enableChangeDataFeed\", \"true\")\n",
    "     .option(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
    "     .option(\"delta.autoOptimize.autoCompact\", \"true\")\n",
    "     .saveAsTable(table_path))\n",
    "    \n",
    "    # Set additional table properties\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {table_path}\n",
    "        SET TBLPROPERTIES (\n",
    "            'delta.enableChangeDataFeed' = 'true',\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact' = 'true'\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    logger.info(f\"Delta table {table_path} created with CDC enabled\")\n",
    "    return table_path\n",
    "\n",
    "# Create Delta table\n",
    "table_path = create_delta_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Databricks Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch databricks-sdk mlflow[databricks]>=2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import time\n",
    "\n",
    "@error_handler\n",
    "def setup_vector_database():\n",
    "    \"\"\"Setup Databricks Vector Database with proper configuration\"\"\"\n",
    "    \n",
    "    # Initialize vector search client\n",
    "    vs_client = VectorSearchClient(disable_notice=True)\n",
    "    \n",
    "    # Create vector search endpoint\n",
    "    try:\n",
    "        endpoint = vs_client.create_endpoint(\n",
    "            name=config.vector_search_endpoint,\n",
    "            endpoint_type=\"STANDARD\"\n",
    "        )\n",
    "        logger.info(f\"Vector search endpoint {config.vector_search_endpoint} created\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            logger.info(f\"Vector search endpoint {config.vector_search_endpoint} already exists\")\n",
    "        else:\n",
    "            raise RAGException(f\"Failed to create vector endpoint: {e}\")\n",
    "    \n",
    "    # Wait for endpoint to be ready\n",
    "    vs_client.wait_for_endpoint(config.vector_search_endpoint)\n",
    "    \n",
    "    # Create vector index\n",
    "    index_name = f\"{config.catalog_name}.{config.schema_name}.{config.vector_index_name}\"\n",
    "    \n",
    "    try:\n",
    "        index = vs_client.create_delta_sync_index(\n",
    "            endpoint_name=config.vector_search_endpoint,\n",
    "            source_table_name=table_path,\n",
    "            index_name=index_name,\n",
    "            pipeline_type=\"TRIGGERED\",\n",
    "            primary_key=\"record_id\",\n",
    "            embedding_source_column=\"Description\",\n",
    "            embedding_model_endpoint_name=config.embedding_model\n",
    "        )\n",
    "        logger.info(f\"Vector index {index_name} created\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            logger.info(f\"Vector index {index_name} already exists\")\n",
    "            index = vs_client.get_index(index_name)\n",
    "        else:\n",
    "            raise RAGException(f\"Failed to create vector index: {e}\")\n",
    "    \n",
    "    # Wait for index to be ready\n",
    "    vs_client.wait_for_index(index_name)\n",
    "    \n",
    "    return vs_client, index\n",
    "\n",
    "# Setup vector database\n",
    "vs_client, vector_index = setup_vector_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. AI Gateway Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "@error_handler\n",
    "def setup_ai_gateway():\n",
    "    \"\"\"Setup AI Gateway for model access\"\"\"\n",
    "    \n",
    "    # Configure AI Gateway (using Azure OpenAI through Databricks AI Gateway)\n",
    "    ai_gateway_client = AzureOpenAI(\n",
    "        api_key=config.azure_openai_key,\n",
    "        api_version=config.azure_openai_api_version,\n",
    "        azure_endpoint=config.azure_openai_endpoint,\n",
    "        # Add AI Gateway specific headers\n",
    "        default_headers={\n",
    "            \"x-databricks-ai-gateway\": \"true\",\n",
    "            \"x-databricks-workspace-id\": spark.conf.get(\"spark.databricks.workspaceUrl\").split(\"/\")[2]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    logger.info(\"AI Gateway client configured\")\n",
    "    return ai_gateway_client\n",
    "\n",
    "# Setup AI Gateway\n",
    "ai_gateway_client = setup_ai_gateway()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MLflow 3.0 Agent Framework Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import pyfunc\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set MLflow registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "class ModernRAGAgent(pyfunc.PythonModel):\n",
    "    \"\"\"Modern RAG Agent using MLflow 3.0 Agent Framework\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.vector_client = None\n",
    "        self.vector_index = None\n",
    "        self.ai_client = None\n",
    "        \n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load model context and dependencies\"\"\"\n",
    "        try:\n",
    "            from databricks.vector_search.client import VectorSearchClient\n",
    "            from openai import AzureOpenAI\n",
    "            \n",
    "            # Initialize vector search client\n",
    "            self.vector_client = VectorSearchClient(disable_notice=True)\n",
    "            \n",
    "            # Get vector index\n",
    "            index_name = f\"{self.config.catalog_name}.{self.config.schema_name}.{self.config.vector_index_name}\"\n",
    "            self.vector_index = self.vector_client.get_index(index_name)\n",
    "            \n",
    "            # Initialize AI Gateway client\n",
    "            self.ai_client = AzureOpenAI(\n",
    "                api_key=self.config.azure_openai_key,\n",
    "                api_version=self.config.azure_openai_api_version,\n",
    "                azure_endpoint=self.config.azure_openai_endpoint\n",
    "            )\n",
    "            \n",
    "            logger.info(\"RAG Agent context loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load context: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def retrieve_context(self, query: str, num_results: int = 3) -> List[Dict]:\n",
    "        \"\"\"Retrieve relevant context using vector search\"\"\"\n",
    "        try:\n",
    "            results = self.vector_index.similarity_search(\n",
    "                query_text=query,\n",
    "                columns=[\"Topic\", \"Description\", \"record_id\"],\n",
    "                num_results=num_results\n",
    "            )\n",
    "            \n",
    "            context_data = results.get('result', {}).get('data_array', [])\n",
    "            logger.info(f\"Retrieved {len(context_data)} context items for query\")\n",
    "            \n",
    "            return context_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Context retrieval failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_response(self, query: str, context: List[Dict]) -> str:\n",
    "        \"\"\"Generate response using AI Gateway\"\"\"\n",
    "        try:\n",
    "            # Format context\n",
    "            context_text = \"\\n\".join([\n",
    "                f\"Topic: {item[0]}\\nDescription: {item[1]}\" \n",
    "                for item in context if len(item) >= 2\n",
    "            ])\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.ai_client.chat.completions.create(\n",
    "                model=self.config.azure_openai_model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\", \n",
    "                        \"content\": \"You are a helpful AI assistant specializing in diabetes care. Use the provided context to answer questions accurately and helpfully.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"Question: {query}\\n\\nContext:\\n{context_text}\\n\\nPlease provide a helpful answer based on the context provided.\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Response generation failed: {e}\")\n",
    "            return f\"I apologize, but I encountered an error while processing your question: {str(e)}\"\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"Main prediction method for MLflow\"\"\"\n",
    "        try:\n",
    "            if isinstance(model_input, pd.DataFrame):\n",
    "                query = model_input[\"query\"].iloc[0]\n",
    "            else:\n",
    "                query = model_input.get(\"query\", \"\")\n",
    "            \n",
    "            if not query:\n",
    "                return \"Please provide a valid query.\"\n",
    "            \n",
    "            # Retrieve context\n",
    "            retrieved_context = self.retrieve_context(query)\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.generate_response(query, retrieved_context)\n",
    "            \n",
    "            # Log inference for monitoring\n",
    "            self._log_inference(query, retrieved_context, response)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed: {e}\")\n",
    "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def _log_inference(self, query: str, context: List[Dict], response: str):\n",
    "        \"\"\"Log inference data for monitoring\"\"\"\n",
    "        try:\n",
    "            inference_data = {\n",
    "                \"timestamp\": pd.Timestamp.now(),\n",
    "                \"query\": query,\n",
    "                \"context_items\": len(context),\n",
    "                \"response_length\": len(response),\n",
    "                \"model_version\": self.config.azure_openai_model\n",
    "            }\n",
    "            \n",
    "            # This would be logged to inference table in production\n",
    "            logger.info(f\"Inference logged: {inference_data}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log inference: {e}\")\n",
    "\n",
    "# Initialize the agent\n",
    "rag_agent = ModernRAGAgent(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Registration with Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def register_model_with_uc():\n",
    "    \"\"\"Register model with Unity Catalog\"\"\"\n",
    "    \n",
    "    # Set experiment\n",
    "    experiment_name = f\"/Users/{spark.sql('SELECT current_user()').collect()[0][0]}/rag_experiments\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"rag_agent_training\") as run:\n",
    "        # Create signature\n",
    "        input_example = pd.DataFrame([{\"query\": \"What is diabetes?\"}])\n",
    "        signature = infer_signature(input_example, \"Sample response about diabetes\")\n",
    "        \n",
    "        # Log model with MLflow\n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"rag_agent\",\n",
    "            python_model=rag_agent,\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            pip_requirements=[\n",
    "                \"databricks-vectorsearch\",\n",
    "                \"openai==1.56.0\",\n",
    "                \"pandas\",\n",
    "                \"numpy\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Register model to Unity Catalog\n",
    "        model_name = f\"{config.catalog_name}.{config.schema_name}.rag_agent_model\"\n",
    "        \n",
    "        registered_model = mlflow.register_model(\n",
    "            model_uri=model_info.model_uri,\n",
    "            name=model_name,\n",
    "            tags={\n",
    "                \"environment\": config.environment,\n",
    "                \"version\": \"v1.0\",\n",
    "                \"framework\": \"mlflow_agent\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Model registered: {model_name}, Version: {registered_model.version}\")\n",
    "        \n",
    "        return registered_model\n",
    "\n",
    "# Register model\n",
    "registered_model = register_model_with_uc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Inference Tables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def setup_inference_tables():\n",
    "    \"\"\"Setup inference tables for monitoring\"\"\"\n",
    "    \n",
    "    inference_table_path = f\"{config.catalog_name}.{config.schema_name}.{config.inference_table_name}\"\n",
    "    \n",
    "    # Create inference table schema\n",
    "    inference_schema = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_path} (\n",
    "            request_id STRING,\n",
    "            timestamp TIMESTAMP,\n",
    "            query STRING,\n",
    "            response STRING,\n",
    "            context_items INT,\n",
    "            response_time_ms BIGINT,\n",
    "            model_version STRING,\n",
    "            endpoint_name STRING,\n",
    "            user_id STRING,\n",
    "            session_id STRING,\n",
    "            feedback_score DOUBLE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "        ) USING DELTA\n",
    "        TBLPROPERTIES (\n",
    "            'delta.enableChangeDataFeed' = 'true',\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true'\n",
    "        )\n",
    "    \"\"\".format(table_path=inference_table_path)\n",
    "    \n",
    "    spark.sql(inference_schema)\n",
    "    logger.info(f\"Inference table {inference_table_path} created\")\n",
    "    \n",
    "    return inference_table_path\n",
    "\n",
    "# Setup inference tables\n",
    "inference_table_path = setup_inference_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Mosaic AI Model Serving Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import *\n",
    "import json\n",
    "\n",
    "@error_handler\n",
    "def setup_mosaic_ai_serving():\n",
    "    \"\"\"Setup Mosaic AI Model Serving with proper configuration\"\"\"\n",
    "    \n",
    "    model_name = f\"{config.catalog_name}.{config.schema_name}.rag_agent_model\"\n",
    "    endpoint_name = config.model_serving_endpoint\n",
    "    \n",
    "    # Configure serving endpoint based on environment\n",
    "    if config.environment == \"prod\" and config.provisioned_throughput:\n",
    "        # Production with provisioned throughput\n",
    "        served_entities = [\n",
    "            ServedEntityInput(\n",
    "                entity_name=model_name,\n",
    "                entity_version=registered_model.version,\n",
    "                workload_size=\"Small\",\n",
    "                scale_to_zero_enabled=False,\n",
    "                workload_type=\"CPU\"\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        # Dev/QA with scale-to-zero\n",
    "        served_entities = [\n",
    "            ServedEntityInput(\n",
    "                entity_name=model_name,\n",
    "                entity_version=registered_model.version,\n",
    "                workload_size=\"Small\",\n",
    "                scale_to_zero_enabled=True,\n",
    "                workload_type=\"CPU\"\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    # Endpoint configuration\n",
    "    endpoint_config = CreateServingEndpoint(\n",
    "        name=endpoint_name,\n",
    "        config=EndpointCoreConfigInput(\n",
    "            served_entities=served_entities,\n",
    "            auto_capture_config=AutoCaptureConfigInput(\n",
    "                catalog_name=config.catalog_name,\n",
    "                schema_name=config.schema_name,\n",
    "                table_name_prefix=config.inference_table_name\n",
    "            )\n",
    "        ),\n",
    "        tags=[\n",
    "            EndpointTag(key=\"environment\", value=config.environment),\n",
    "            EndpointTag(key=\"team\", value=\"rag-team\"),\n",
    "            EndpointTag(key=\"cost-center\", value=\"ai-ml\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Create serving endpoint\n",
    "        workspace_client.serving_endpoints.create(endpoint_config)\n",
    "        logger.info(f\"Mosaic AI serving endpoint {endpoint_name} created\")\n",
    "        \n",
    "        # Wait for endpoint to be ready\n",
    "        workspace_client.serving_endpoints.wait_for_endpoint(endpoint_name)\n",
    "        logger.info(f\"Endpoint {endpoint_name} is ready\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            logger.info(f\"Endpoint {endpoint_name} already exists\")\n",
    "            # Update endpoint with new model version\n",
    "            workspace_client.serving_endpoints.update_config(\n",
    "                name=endpoint_name,\n",
    "                served_entities=served_entities\n",
    "            )\n",
    "        else:\n",
    "            raise RAGException(f\"Failed to create serving endpoint: {e}\")\n",
    "\n",
    "# Setup Mosaic AI serving\n",
    "setup_mosaic_ai_serving()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Lakehouse Monitoring for GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service.catalog import MonitorInfo, MonitorInferenceLog\n",
    "\n",
    "@error_handler\n",
    "def setup_lakehouse_monitoring():\n",
    "    \"\"\"Setup Lakehouse Monitoring for GenAI applications\"\"\"\n",
    "    \n",
    "    monitor_name = f\"{config.catalog_name}.{config.schema_name}.rag_model_monitor\"\n",
    "    \n",
    "    try:\n",
    "        # Create monitor for the inference table\n",
    "        monitor_config = MonitorInfo(\n",
    "            table_name=inference_table_path,\n",
    "            baseline_table_name=None,  # No baseline for inference monitoring\n",
    "            monitor_type=\"InferenceLogs\",\n",
    "            output_schema_name=f\"{config.catalog_name}.{config.schema_name}\",\n",
    "            inference_log=MonitorInferenceLog(\n",
    "                granularities=[\"1 hour\", \"1 day\"],\n",
    "                model_id_col=\"model_version\",\n",
    "                prediction_col=\"response\",\n",
    "                timestamp_col=\"timestamp\",\n",
    "                problem_type=\"TEXT_CLASSIFICATION\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # This would be used with the actual monitoring API\n",
    "        logger.info(f\"Lakehouse monitoring configured for {inference_table_path}\")\n",
    "        \n",
    "        # Setup alerts for monitoring\n",
    "        setup_monitoring_alerts()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Monitoring setup warning: {e}\")\n",
    "\n",
    "def setup_monitoring_alerts():\n",
    "    \"\"\"Setup monitoring alerts for model performance\"\"\"\n",
    "    \n",
    "    alert_queries = {\n",
    "        \"high_latency\": f\"\"\"\n",
    "            SELECT \n",
    "                AVG(response_time_ms) as avg_latency,\n",
    "                COUNT(*) as request_count\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
    "        \"\"\",\n",
    "        \n",
    "        \"low_feedback_score\": f\"\"\"\n",
    "            SELECT \n",
    "                AVG(feedback_score) as avg_feedback,\n",
    "                COUNT(*) as request_count\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
    "                AND feedback_score IS NOT NULL\n",
    "        \"\"\",\n",
    "        \n",
    "        \"error_rate\": f\"\"\"\n",
    "            SELECT \n",
    "                SUM(CASE WHEN response LIKE '%error%' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as error_rate\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Monitoring alerts configured\")\n",
    "\n",
    "# Setup monitoring\n",
    "setup_lakehouse_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Serverless ETL Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def create_serverless_etl_job():\n",
    "    \"\"\"Create serverless job for ETL pipeline\"\"\"\n",
    "    \n",
    "    job_config = {\n",
    "        \"name\": config.serverless_job_name,\n",
    "        \"format\": \"MULTI_TASK\",\n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"task_key\": \"data_ingestion\",\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/path/to/data_ingestion_notebook\",\n",
    "                    \"base_parameters\": {\n",
    "                        \"catalog_name\": config.catalog_name,\n",
    "                        \"schema_name\": config.schema_name,\n",
    "                        \"environment\": config.environment\n",
    "                    }\n",
    "                },\n",
    "                \"job_cluster_key\": \"serverless_cluster\"\n",
    "            },\n",
    "            {\n",
    "                \"task_key\": \"vector_index_sync\",\n",
    "                \"depends_on\": [{\"task_key\": \"data_ingestion\"}],\n",
    "                \"python_wheel_task\": {\n",
    "                    \"package_name\": \"rag_pipeline\",\n",
    "                    \"entry_point\": \"sync_vector_index\",\n",
    "                    \"parameters\": [\n",
    "                        \"--catalog\", config.catalog_name,\n",
    "                        \"--schema\", config.schema_name,\n",
    "                        \"--index\", config.vector_index_name\n",
    "                    ]\n",
    "                },\n",
    "                \"job_cluster_key\": \"serverless_cluster\"\n",
    "            }\n",
    "        ],\n",
    "        \"job_clusters\": [\n",
    "            {\n",
    "                \"job_cluster_key\": \"serverless_cluster\",\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                    \"node_type_id\": \"i3.xlarge\",\n",
    "                    \"num_workers\": 0,  # Serverless\n",
    "                    \"spark_conf\": {\n",
    "                        \"spark.databricks.cluster.profile\": \"serverless\",\n",
    "                        \"spark.databricks.sql.initial.catalog.name\": config.catalog_name\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"schedule\": {\n",
    "            \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n",
    "            \"timezone_id\": \"UTC\"\n",
    "        },\n",
    "        \"email_notifications\": {\n",
    "            \"on_failure\": [\"your-email@company.com\"],\n",
    "            \"on_success\": [\"your-email@company.com\"]\n",
    "        },\n",
    "        \"tags\": {\n",
    "            \"environment\": config.environment,\n",
    "            \"team\": \"rag-team\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Serverless ETL job {config.serverless_job_name} configuration created\")\n",
    "    return job_config\n",
    "\n",
    "# Create ETL job configuration\n",
    "etl_job_config = create_serverless_etl_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Genie Integration for Text2SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def setup_genie_integration():\n",
    "    \"\"\"Setup Genie for Text2SQL capabilities\"\"\"\n",
    "    \n",
    "    # Create Genie space for diabetes data\n",
    "    genie_config = {\n",
    "        \"space_name\": \"diabetes_rag_space\",\n",
    "        \"description\": \"RAG system data for diabetes FAQ queries\",\n",
    "        \"tables\": [\n",
    "            {\n",
    "                \"table_name\": table_path,\n",
    "                \"description\": \"Diabetes FAQ data with topics and descriptions\"\n",
    "            },\n",
    "            {\n",
    "                \"table_name\": inference_table_path,\n",
    "                \"description\": \"Model inference logs and monitoring data\"\n",
    "            }\n",
    "        ],\n",
    "        \"instructions\": [\n",
    "            \"This space contains diabetes-related FAQ data\",\n",
    "            \"Use this data to answer questions about diabetes care and treatment\",\n",
    "            \"The inference table contains model performance metrics\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # SQL queries for Genie to understand the data better\n",
    "    sample_queries = [\n",
    "        f\"SELECT COUNT(*) FROM {table_path}\",\n",
    "        f\"SELECT Topic, COUNT(*) FROM {table_path} GROUP BY Topic\",\n",
    "        f\"SELECT * FROM {inference_table_path} WHERE timestamp >= current_date()\",\n",
    "        f\"SELECT AVG(response_time_ms) FROM {inference_table_path} WHERE timestamp >= current_date()\"\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"Genie integration configured for Text2SQL capabilities\")\n",
    "    return genie_config\n",
    "\n",
    "# Setup Genie integration\n",
    "genie_config = setup_genie_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Cost and Latency Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def implement_cost_optimization():\n",
    "    \"\"\"Implement cost and latency optimization strategies\"\"\"\n",
    "    \n",
    "    optimization_strategies = {\n",
    "        \"caching\": {\n",
    "            \"enabled\": True,\n",
    "            \"cache_ttl_minutes\": 60,\n",
    "            \"cache_table\": f\"{config.catalog_name}.{config.schema_name}.query_cache\"\n",
    "        },\n",
    "        \"request_batching\": {\n",
    "            \"enabled\": True,\n",
    "            \"batch_size\": 10,\n",
    "            \"batch_timeout_ms\": 1000\n",
    "        },\n",
    "        \"auto_scaling\": {\n",
    "            \"scale_to_zero\": config.environment in [\"dev\", \"qa\"],\n",
    "            \"min_instances\": 0 if config.environment in [\"dev\", \"qa\"] else 1,\n",
    "            \"max_instances\": 5 if config.environment == \"prod\" else 2\n",
    "        },\n",
    "        \"model_optimization\": {\n",
    "            \"use_quantization\": True,\n",
    "            \"optimize_for_latency\": config.environment == \"prod\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create cache table for frequently asked questions\n",
    "    cache_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {optimization_strategies['caching']['cache_table']} (\n",
    "            query_hash STRING,\n",
    "            query TEXT,\n",
    "            response TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
    "            access_count BIGINT DEFAULT 1,\n",
    "            last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "        ) USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(cache_table_sql)\n",
    "    logger.info(\"Cost optimization strategies implemented\")\n",
    "    \n",
    "    return optimization_strategies\n",
    "\n",
    "# Implement optimizations\n",
    "optimization_config = implement_cost_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Automated Model Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def create_automated_deployment_pipeline():\n",
    "    \"\"\"Create automated deployment pipeline\"\"\"\n",
    "    \n",
    "    deployment_pipeline = {\n",
    "        \"stages\": [\n",
    "            {\n",
    "                \"name\": \"model_validation\",\n",
    "                \"type\": \"validation\",\n",
    "                \"criteria\": {\n",
    "                    \"accuracy_threshold\": 0.85,\n",
    "                    \"latency_threshold_ms\": 2000,\n",
    "                    \"error_rate_threshold\": 0.05\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"staging_deployment\",\n",
    "                \"type\": \"deployment\",\n",
    "                \"environment\": \"staging\",\n",
    "                \"approval_required\": False,\n",
    "                \"auto_promote\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"production_deployment\",\n",
    "                \"type\": \"deployment\", \n",
    "                \"environment\": \"production\",\n",
    "                \"approval_required\": True,\n",
    "                \"auto_promote\": False,\n",
    "                \"canary_percentage\": 10\n",
    "            }\n",
    "        ],\n",
    "        \"triggers\": [\n",
    "            {\n",
    "                \"type\": \"model_registration\",\n",
    "                \"condition\": \"new_version\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"schedule\",\n",
    "                \"cron\": \"0 0 * * 1\"  # Weekly on Monday\n",
    "            }\n",
    "        ],\n",
    "        \"notifications\": {\n",
    "            \"slack_webhook\": \"your-slack-webhook-url\",\n",
    "            \"email_list\": [\"ml-team@company.com\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Automated deployment pipeline configured\")\n",
    "    return deployment_pipeline\n",
    "\n",
    "# Create deployment pipeline\n",
    "deployment_pipeline = create_automated_deployment_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Dashboard for Monitoring and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def create_monitoring_dashboard():\n",
    "    \"\"\"Create Databricks dashboard for monitoring\"\"\"\n",
    "    \n",
    "    dashboard_queries = {\n",
    "        \"request_volume\": f\"\"\"\n",
    "            SELECT \n",
    "                date_trunc('hour', timestamp) as hour,\n",
    "                COUNT(*) as request_count\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
    "            GROUP BY hour\n",
    "            ORDER BY hour\n",
    "        \"\"\",\n",
    "        \n",
    "        \"average_latency\": f\"\"\"\n",
    "            SELECT \n",
    "                date_trunc('hour', timestamp) as hour,\n",
    "                AVG(response_time_ms) as avg_latency_ms,\n",
    "                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95_latency_ms\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
    "            GROUP BY hour\n",
    "            ORDER BY hour\n",
    "        \"\"\",\n",
    "        \n",
    "        \"feedback_scores\": f\"\"\"\n",
    "            SELECT \n",
    "                date_trunc('day', timestamp) as day,\n",
    "                AVG(feedback_score) as avg_feedback,\n",
    "                COUNT(CASE WHEN feedback_score >= 4 THEN 1 END) * 100.0 / COUNT(*) as satisfaction_rate\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 7 DAYS\n",
    "                AND feedback_score IS NOT NULL\n",
    "            GROUP BY day\n",
    "            ORDER BY day\n",
    "        \"\"\",\n",
    "        \n",
    "        \"top_queries\": f\"\"\"\n",
    "            SELECT \n",
    "                query,\n",
    "                COUNT(*) as frequency,\n",
    "                AVG(response_time_ms) as avg_latency\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 7 DAYS\n",
    "            GROUP BY query\n",
    "            ORDER BY frequency DESC\n",
    "            LIMIT 10\n",
    "        \"\"\",\n",
    "        \n",
    "        \"error_analysis\": f\"\"\"\n",
    "            SELECT \n",
    "                CASE \n",
    "                    WHEN response LIKE '%error%' THEN 'Error'\n",
    "                    WHEN response LIKE '%sorry%' THEN 'Apology'\n",
    "                    ELSE 'Success'\n",
    "                END as response_type,\n",
    "                COUNT(*) as count,\n",
    "                COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage\n",
    "            FROM {inference_table_path}\n",
    "            WHERE timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
    "            GROUP BY response_type\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Monitoring dashboard queries created\")\n",
    "    return dashboard_queries\n",
    "\n",
    "# Create dashboard\n",
    "dashboard_queries = create_monitoring_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Testing the Complete RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def test_rag_system():\n",
    "    \"\"\"Test the complete RAG system\"\"\"\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is diabetes?\",\n",
    "        \"How to manage blood sugar levels?\",\n",
    "        \"What are the symptoms of diabetes?\",\n",
    "        \"Diet recommendations for diabetics\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for query in test_queries:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test the agent\n",
    "            input_data = pd.DataFrame([{\"query\": query}])\n",
    "            response = rag_agent.predict(None, input_data)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            result = {\n",
    "                \"query\": query,\n",
    "                \"response\": response,\n",
    "                \"latency_ms\": latency_ms,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Query: {query}\")\n",
    "            logger.info(f\"Response: {response[:100]}...\")\n",
    "            logger.info(f\"Latency: {latency_ms:.2f}ms\")\n",
    "            logger.info(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"query\": query,\n",
    "                \"response\": f\"Error: {str(e)}\",\n",
    "                \"latency_ms\": 0,\n",
    "                \"success\": False\n",
    "            }\n",
    "            logger.error(f\"Failed to process query '{query}': {e}\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the system\n",
    "test_results = test_rag_system()\n",
    "\n",
    "# Display results\n",
    "for result in test_results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Success: {result['success']}\")\n",
    "    print(f\"Latency: {result['latency_ms']:.2f}ms\")\n",
    "    print(f\"Response: {result['response'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Production Deployment Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Deployment Checklist\n",
    "\n",
    "✅ **Infrastructure Setup**\n",
    "- Unity Catalog configured with proper governance\n",
    "- Volumes for unstructured data storage\n",
    "- Serverless warehouse for structured queries\n",
    "- Service principal authentication\n",
    "\n",
    "✅ **Vector Database**\n",
    "- Databricks Vector Search with STANDARD endpoint\n",
    "- Delta sync index with change data feed\n",
    "- Automated embedding generation\n",
    "\n",
    "✅ **Model Serving**\n",
    "- Mosaic AI serving endpoint with auto-capture\n",
    "- Scale-to-zero for dev/QA environments\n",
    "- Provisioned throughput for production\n",
    "- Inference tables for monitoring\n",
    "\n",
    "✅ **Monitoring & Observability**\n",
    "- Lakehouse monitoring for GenAI metrics\n",
    "- Real-time dashboards for performance tracking\n",
    "- Automated alerts for anomaly detection\n",
    "- Cost optimization strategies\n",
    "\n",
    "✅ **MLOps & Automation**\n",
    "- MLflow 3.0 Agent Framework\n",
    "- Automated model deployment pipeline\n",
    "- Version control and model registry\n",
    "- CI/CD integration ready\n",
    "\n",
    "✅ **Data Pipeline**\n",
    "- Serverless ETL jobs for data processing\n",
    "- Change data capture for real-time updates\n",
    "- Data quality monitoring\n",
    "- Genie integration for Text2SQL\n",
    "\n",
    "### Next Steps for Production:\n",
    "1. Configure proper authentication credentials\n",
    "2. Set up monitoring alerts and notifications  \n",
    "3. Implement A/B testing framework\n",
    "4. Add comprehensive logging and audit trails\n",
    "5. Set up backup and disaster recovery\n",
    "6. Configure multi-region deployment if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Configuration Summary\n",
    "\n",
    "Print all configured components and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_deployment_summary():\n",
    "    \"\"\"Print summary of all configured components\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODERN RAG SYSTEM DEPLOYMENT SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    components = {\n",
    "        \"Unity Catalog\": {\n",
    "            \"Catalog\": f\"{config.catalog_name}\",\n",
    "            \"Schema\": f\"{config.schema_name}\",\n",
    "            \"Volume\": f\"{config.volume_name}\",\n",
    "            \"Status\": \"✅ Configured\"\n",
    "        },\n",
    "        \"Data Pipeline\": {\n",
    "            \"Delta Table\": table_path,\n",
    "            \"CDC Enabled\": \"✅ Yes\",\n",
    "            \"Serverless Warehouse\": config.serverless_warehouse,\n",
    "            \"ETL Job\": config.serverless_job_name\n",
    "        },\n",
    "        \"Vector Database\": {\n",
    "            \"Endpoint\": config.vector_search_endpoint,\n",
    "            \"Index\": f\"{config.catalog_name}.{config.schema_name}.{config.vector_index_name}\",\n",
    "            \"Embedding Model\": config.embedding_model,\n",
    "            \"Status\": \"✅ Ready\"\n",
    "        },\n",
    "        \"Model Serving\": {\n",
    "            \"Endpoint\": config.model_serving_endpoint,\n",
    "            \"Model\": f\"{config.catalog_name}.{config.schema_name}.rag_agent_model\",\n",
    "            \"Version\": registered_model.version,\n",
    "            \"Environment\": config.environment,\n",
    "            \"Scale-to-Zero\": \"✅ Yes\" if config.environment in [\"dev\", \"qa\"] else \"❌ No\"\n",
    "        },\n",
    "        \"Monitoring\": {\n",
    "            \"Inference Table\": inference_table_path,\n",
    "            \"Lakehouse Monitoring\": \"✅ Configured\",\n",
    "            \"Dashboard\": \"✅ Ready\",\n",
    "            \"Alerts\": \"✅ Configured\"\n",
    "        },\n",
    "        \"AI Gateway\": {\n",
    "            \"OpenAI Integration\": \"✅ Configured\",\n",
    "            \"Model\": config.azure_openai_model,\n",
    "            \"API Version\": config.azure_openai_api_version\n",
    "        },\n",
    "        \"Additional Features\": {\n",
    "            \"Genie Text2SQL\": \"✅ Configured\",\n",
    "            \"Cost Optimization\": \"✅ Enabled\",\n",
    "            \"Automated Deployment\": \"✅ Ready\",\n",
    "            \"Service Principal Auth\": \"✅ Configured\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for component, details in components.items():\n",
    "        print(f\"\\n{component}:\")\n",
    "        print(\"-\" * len(component))\n",
    "        for key, value in details.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🚀 RAG SYSTEM READY FOR PRODUCTION DEPLOYMENT!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Print deployment summary\n",
    "print_deployment_summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
